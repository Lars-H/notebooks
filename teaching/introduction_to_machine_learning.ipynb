{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning Exercise: Decision Trees and K-Means\n",
    "\n",
    "In this notebook, you will learn about supervised and unsupervised machine learning models. In this version, you will learn about Decision Tress and K-Means as presented in the Lecture.\n",
    "\n",
    "- You will be guided through the note step by step. \n",
    "- You can execute the cells in a sequential order to follow the storyline.\n",
    "- We indicated **\"Interactive Task\"** throughout the notebook. These are parts, were you might need to do small adjustements to the code to see how the results change.\n",
    "- However, feel free to also adjust and play around with other parts as you go along.\n",
    "\n",
    "Let us start by importing all the libraries that we will need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import tree\n",
    "import graphviz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "First, we learn how to apply decision tress while working on the \"Titanic Challenge\". More specifically, we use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
    "\n",
    "The Titanic Challenge is a widely used common machine learning beginner challenge. It is also the getting started competition at [Kaggle](https://www.kaggle.com) - the online community of data scientists and machine learners. So, after working through this notebook, you can quickly work on your own, improved solution, join the Kaggle competition, and compare your results with the community - give it a try!\n",
    "\n",
    "We are going to use the python library `scikit-learn`. Scikit-learn is built on NumPy, SciPy, and matplotlib and provides a range of supervised and unsupervised learning algorithms for classification, regression, and clustering. For more information about scikit-learn, click [here](https://scikit-learn.org/stable/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The file `titanic_data.csv` contains data from Titanic passengers with the following columns:\n",
    "\n",
    "- PassengerId: unique identification number for each passenger\n",
    "- Survived: 0 = No, 1 = Yes\n",
    "- Pclass: the passenger class, 1 = 1st (upperclass), 2 = 2nd (middle class), 3 = 3rd (lower class)\n",
    "- Name: full name of the passenger\n",
    "- Sex: male or female\n",
    "- Age: age of passenger in years, e.g. 22\n",
    "- SibSp: number of siblings/ spouses aboard the Titanic\n",
    "- Parch: number of parents/ children aboard the Titanic\n",
    "- Ticket: ticket number\n",
    "- Fare: passenger fare  \n",
    "- Cabin: cabin number\n",
    "- Embarked: port of embarkation, C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "Let's import the data and have a closer look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://people.aifb.kit.edu/zg2916/teaching/hector2020/titanic_data.csv\"\n",
    "titanic_data = pd.read_csv(data_url)\n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always essential to have a closer look at the data first. Here are some of the questions you may want to look at more precisely. \n",
    "- Are the features numeric or categorical?\n",
    "- Which features may influence the prediction, and which not?\n",
    "- Which features have to be proceeded? \n",
    "- Do we have missing entries? (NaN)\n",
    "    \n",
    "It is also a good idea to plot some of the features to generate a good understanding of the data and decide for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Class - Survived\n",
    "sns.barplot(x='Pclass', y='Survived', data=titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Fare Distribution\n",
    "sns.distplot(titanic_data['Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above we can see that class influences the chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sex - Survived\n",
    "sns.barplot(x='Sex', y='Survived', data=titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, gender impacts the chance of survival.\n",
    "\n",
    "So the column `Survived` is our prediction target. For our first try, let's focus on the features `Pclass`, `Sex`, `Age`, `Fare` and `Embarked`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, we have to drop the features we don't want to take into consideration for the prediction.\n",
    "Here, we assume the following columns to have no predictive power:\n",
    "- PassengerId\n",
    "- Name\n",
    "- SibSp (Sibling or Spouse Aboad)\n",
    "- Parch (Number of parents/siblings aboard)\n",
    "- Cabin\n",
    "- Ticket (ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns from the dataframe\n",
    "data = titanic_data.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Cabin', 'Ticket'], axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find out about missing entries in the remaining data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = data.isnull().sum()/data.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Age is missing 177 entries, and Embarked is missing 2 entries. We are going to fill the 2 missing entries of Embarked with the most common one and the 177 entires of Age with the average age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the most common entry:\n",
    "data['Embarked'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values of Embarked with the most common entry\n",
    "common_value = 'S'\n",
    "data['Embarked'] = data['Embarked'].fillna(common_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values of Age with the mean Age\n",
    "\n",
    "# Compute the mean age\n",
    "mean = data['Age'].mean()\n",
    "\n",
    "# Fill Na values\n",
    "data['Age'] = data['Age'].fillna(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Task\n",
    "\n",
    "- Can you think about other ways to handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above in data.info(), we have different datatypes. We need to transform the categorical values of Sex and Embarked into numerical ones so that machine learning algorithms can process them. Furthermore, the features have different ranges that we need to convert into roughly the same scale. In our case, we need to categorize the feature Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex\n",
    "genders = {\"male\": 0, \"female\": 1}\n",
    "data['Sex'] = data['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embarked\n",
    "ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "data['Embarked'] = data['Embarked'].map(ports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we categorize Age and convert it from float to int. For the categorization, it is crucial to distribute the values, and not have 90 % of the data in one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age\n",
    "data['Age'] = data['Age'].astype(int)\n",
    "data.loc[ data['Age'] <= 11, 'Age'] = 0\n",
    "data.loc[(data['Age'] > 11) & (data['Age'] <= 18), 'Age'] = 1\n",
    "data.loc[(data['Age'] > 18) & (data['Age'] <= 22), 'Age'] = 2\n",
    "data.loc[(data['Age'] > 22) & (data['Age'] <= 27), 'Age'] = 3\n",
    "data.loc[(data['Age'] > 27) & (data['Age'] <= 33), 'Age'] = 4\n",
    "data.loc[(data['Age'] > 33) & (data['Age'] <= 40), 'Age'] = 5\n",
    "data.loc[(data['Age'] > 40) & (data['Age'] <= 66), 'Age'] = 6\n",
    "data.loc[ data['Age'] > 66, 'Age'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the distribution\n",
    "data['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a last look at our data before getting started with ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test\n",
    "\n",
    "Now we want to split the data into train and test data. There are many ways and methods to split the data. We will simply use a test size of 10 % of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to separate the target column first:\n",
    "X = data.drop(\"Survived\", axis=1)\n",
    "y = data[\"Survived\"]\n",
    "\n",
    "# And split the data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Tasks:\n",
    "\n",
    "- Try out other split ratios (`test_size` parameter) and see how the results of your model change. Let us start with a value of `0.1` and change it once we have investigated the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier object with a depth of \"max_depth\"\n",
    "decision_tree = DecisionTreeClassifier(max_depth=3) \n",
    "\n",
    "# See here for reference: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html \n",
    "# There you can find also additional parameters\n",
    "\n",
    "# Train the model using the training data\n",
    "decision_tree.fit(X_train, y_train)  \n",
    "\n",
    "# Compute the predictions for the testing data\n",
    "y_pred = decision_tree.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "We can get a first insight in the performance of the model by looking at the confusion matrix.\n",
    "The confusion matrix combines the value for *True Positives (TP)*, *True Negatives (TN)*, *False Negatives (FN)* and *False Positives*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define a function to compute these values based on the a given prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute TP, TN, FP, FN values\n",
    "def confusion_matrix(y_test, y_pred):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    #the true values:\n",
    "    y_real = np.array(y_test)\n",
    "    \n",
    "    # For all predictions made\n",
    "    for i in range(len(y_pred)):\n",
    "        # True Positives\n",
    "        if y_pred[i] == 1 and  y_real[i] == 1:\n",
    "            tp += 1\n",
    "            \n",
    "        # True Negatives\n",
    "        elif y_pred[i] == 0 and  y_real[i] == 0:\n",
    "            tn += 1\n",
    "        \n",
    "        # False Positives\n",
    "        elif y_pred[i] == 1 and  y_real[i] == 0:\n",
    "            fp += 1\n",
    "            \n",
    "        # False Negative\n",
    "        elif y_pred[i] == 0 and  y_real[i] == 1:\n",
    "            fn += 1\n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the values for a given prediction on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn, tn = confusion_matrix(y_test, y_pred)\n",
    "print(f\"True Positives shows how many passengers correctly classified as survived: {tp}\")\n",
    "print(f\"True Negatives shows how many passengers correctly classified as dead: {tn}\")\n",
    "print(f\"False Negatives shows how many passengers wrongly classified as dead: {fn}\")\n",
    "print(f\"False Positives shows how many passengers wrongly classified as survived: {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these values are all absolute values. As a result, the predicitions for different sizes of test sets are not comparable.\n",
    "In order to get a better insight into the performance, we introduce the following metrics to get a better idea about the performance of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1 Score\n",
    "\n",
    "Precision, recall and F1 Score are the commonly used evaluation metrics for classification task.\n",
    "We can compute them based on the previously computed TP, TN, FN, and FP values using the following formulas:\n",
    "\n",
    "$Precsion = \\frac{TP}{(TP+FP)}$\n",
    "\n",
    "$Recall = \\frac{TP}{(TP+FN)}$\n",
    "\n",
    "$F_1 = \\frac{(2 * Precision * Recall)}{ (Precision + Recall)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Task\n",
    "\n",
    "- In the code box below, add the code to compute Precision, Recall and F1 Measure using the formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,tn,fp,fn):\n",
    "    precision = 0 # ADD CODE HERE \n",
    "    return precision\n",
    "\n",
    "def recall(tp,tn,fp,fn):\n",
    "    recall = 0 # ADD CODE HERE \n",
    "    return recall\n",
    "\n",
    "def f1(tp,tn,fp,fn):\n",
    "    precision = 0 # ADD CODE HERE (Hint: reuse existing functions)\n",
    "    recall = 0 # ADD CODE HERE (Hint: reuse existing functions)\n",
    "    f1 = 0 # ADD CODE HERE \n",
    "    return f1\n",
    "\n",
    "print('Precision:', round(precision(tp, fp, tn, fn),3))\n",
    "print('Recall:', round(recall(tp, fp, tn, fn), 3))\n",
    "print('F1:', round(f1(tp, fp, tn, fn), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the formulas that are provided as part of the `sklearn` library to compute those values.\n",
    "\n",
    "### Interactive Task:\n",
    "\n",
    "- Check wehther the values you compute are identical to those using the library fromulas below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_val = precision_score(y_test, y_pred)\n",
    "recall_val = recall_score(y_test, y_pred)\n",
    "f1_val = f1_score(y_test, y_pred)\n",
    "print('Precision:', round(precision_val,3))\n",
    "print('Recall:', round(recall_val, 3))\n",
    "print('F1:', round(f1_val, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also visualize the results as bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=[\"precision\", \"recall\", \"F1\"], y=[precision_val, recall_val, f1_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Decision Tree\n",
    "\n",
    "We want to get a better understanding of the decision trees and the rules that it has learned.\n",
    "In order to do so, we can visualize the decision tree to inspect its decision rules.\n",
    "\n",
    "Each node in the graph represents a decision rule. The nodes include information on:\n",
    "- The feature\n",
    "- The importance of the feature (as measured by the Gini Importance metric)\n",
    "- The number of sample to which the rule was applied\n",
    "- The values: How many sample fit into either class\n",
    "- The class which would be predicited at the current point in the tree\n",
    "\n",
    "### Interactive Tasks:\n",
    "\n",
    "- Visualize the decision tree for different depth values. To do so, go back to the cell in which the decision tree is learned and change its parameter \"max_depth\" and re-run the cell\n",
    "- Start with a small value and see how the complexity of the decision rules increase with increasing height.\n",
    "- Check in the visualization which nodes are irrelevant for the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(decision_tree, out_file=None, \n",
    "                     feature_names=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked'],  \n",
    "                     class_names=[\"Survived\", \"Not_Survived\"],  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Tasks\n",
    "\n",
    "After you inspected the decision trees and tried out different parameters, feel free to try the following changes as well:\n",
    "- What happens when you use the actual age instead to the age-class for your predictions? (You will need to re-load the data and skip the step in which the age values are put into classes.)\n",
    "- Do you think there are still features in the data that might not have a predictive power?\n",
    "- What other feature can you think of, that could help the classification?\n",
    "- Should precision and recall always be considered equally important? Can you think of cases where one or the other is more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "After looking at a supervised learning approach to classify data, let us take a closer look at unsupervised learning.\n",
    "In particular, we want to investigate data about customers of a mall.\n",
    "\n",
    "Image you have selected different data about the customers that visit your mall using a survey.\n",
    "Now you want to improve your sales and marketing strategy. In order to do so, you want to identfy different class/clusters of shoppers that are similar in their shopping behaviour.\n",
    "In your survey, you have collected the following data:\n",
    "\n",
    "- Gender\n",
    "- Age\n",
    "- Annual Income\n",
    "- Spending Score (Relative score how much the customer spents when visit the mall)\n",
    "\n",
    "Let us start by loading and checking out our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the newdataset\n",
    "data_url = \"http://people.aifb.kit.edu/zg2916/teaching/hector2020/customer_data.csv\"\n",
    "customer_data = pd.read_csv(data_url)\n",
    "customer_data = customer_data.set_index(\"CustomerID\")\n",
    "customer_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news: All customers complete filled out the surveys, so there are no `null` values which you need to handle.\n",
    "\n",
    "Let us take a look at the data before we start our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we should replace the non-numeric values for gender again and represent gender with a numeric value (0=Male, 1=Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"Male\": 0, \"Female\": 1}\n",
    "customer_data['Gender'] = customer_data['Gender'].map(genders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "Let us visualize the data to get some further insights into the distribution of the values. Maybe we can already see what potential customer clusters/segments can be.\n",
    "\n",
    "Let us first check how the spending score and the income of the customers are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Annual Income (k$)\", y = \"Spending Score (1-100)\", data=customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also introduce a third dimension in our visulaization. For example, we can indicate the age of the customers using a color scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Annual Income (k$)\", y = \"Spending Score (1-100)\", hue=\"Age\", data=customer_data) # Adding a hue for Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a first idea about the data, we want to cluster the customers.\n",
    "\n",
    "For simplicity, let us consider the \"income\" and the \"spending score\" as our features to cluster the customers.\n",
    "Since we K-Means is an unsupervised approach, we do not need to split our data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data according to columns\n",
    "X = customer_data[['Annual Income (k$)','Spending Score (1-100)']]\n",
    "X_array = X.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "After selecting the data, we can apply the K-Means clustering algorithm.\n",
    "\n",
    "Even though K-Means is an unsupervised learning algorithm, we will need to specify how many clusters we want the algorithm to identify.\n",
    "Let us start by trying 3 different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the data\n",
    "kmeans = KMeans(3, init='random') #, n_init=1, max_iter=1)\n",
    "\n",
    "# Get Cluster labels to color data in the plot\n",
    "labels = kmeans.fit_predict(X_array)\n",
    "\n",
    "# Get the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Plot the data points and use the cluster labels to color the data \n",
    "plt.scatter(X_array[:, 0], X_array[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n",
    "\n",
    "# Plot the Centers of the clutsers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive tasks\n",
    "\n",
    "- Try out different number of clusters and check how the clusters changes\n",
    "- What do you think would be a good number of clusters and why? Think about what the data represents and what clusters you would expect in the data?\n",
    "- Once you have clustered your customers, how could you make use of this information? (Think about sales and marketing strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization steps\n",
    "\n",
    "The K-Means algorithm starts by initializing random cluster centers. \n",
    "We can also visualize these initial steps by allowing the algorithm to perform only one iteration (Guessing centers and compute the inertia).\n",
    "\n",
    "\n",
    "### Interactive tasks\n",
    "\n",
    "- Re-run the following cell to see how the random initial state of the algorithm changes.\n",
    "- Increase the `max_iter` parameter to allow the algorithm to execute more iterations. After how many iterations does the algorithm converge regardless of its inital state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the data\n",
    "kmeans = KMeans(3, init='random', n_init=1, max_iter=1)\n",
    "\n",
    "# Get Cluster labels to color data in the plot\n",
    "labels = kmeans.fit_predict(X_array)\n",
    "\n",
    "# Get the coordinates of the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Plot the data points and use the cluster labels to color the data \n",
    "plt.scatter(X_array[:, 0], X_array[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');\n",
    "\n",
    "# Plot the Centers of the clutsers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
